---
layout: post
title: "Back-of-the-Envelope: Attention is All You Need"
date: 2024-09-03
---
"Attention is All You Need" demands attention because of its profound impact. Given its impact in LLM development, the transformer with attention is a bona fide `e=m*c^2` of our time.

![viz](/blog/assets/2024/attention/graphic-viz.png)

At the moment, I am unable to do the topic more justice then the attached resources.

Good luck, and I hope your attention mechanism finds the right weights to understand the topic!

## Resources (in order of comprehensability)

### Python code, mathematics and individual topics developed
[Peter Bloem](https://peterbloem.nl/blog/transformers)

### Visualization and Tailored Examples
[Souvik Mandal](https://itnext.io/attention-is-all-you-need-e8109d2693e)

### Solid Commentary on Original Paper
[Yannic Kilcher](https://www.youtube.com/watch?v=iDulhoQ2pro)

### Original Paper
[Attention is All You Need](/blog/assets/2024/attention/0-ATTN-0.pdf)

### Jax implementation of transformers
[Saurabh Alone AI](https://github.com/saurabhaloneai/Llama-3-From-Scratch-In-Pure-Jax)